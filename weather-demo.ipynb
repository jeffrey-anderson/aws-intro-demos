{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('weather').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"s3://noaa-ghcn-pds/ghcnd-stations.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "\n",
    "stations = df.select(\n",
    "    df.value.substr(1,11).alias('ID'),\n",
    "    df.value.substr(13,8).cast('double').alias('LATITUDE'),\n",
    "    df.value.substr(22,9).cast('double').alias('LONGITUDE'),\n",
    "    df.value.substr(32,6).cast('double').alias('ELEVATION'),\n",
    "    trim(df.value.substr(39,2)).alias('STATE'),\n",
    "    trim(df.value.substr(42,30)).alias('NAME'),\n",
    "    trim(df.value.substr(73,3)).alias('GSN_FLAG'),\n",
    "    trim(df.value.substr(77,3)).alias('NETWORK_FLAG'),\n",
    "    trim(df.value.substr(81,5)).alias('WMO_ID')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.filter((\"NAME like '%COLUMBUS%' AND STATE = 'OH'\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.filter((\"NAME like '%PORT COLUMBUS%' AND STATE = 'OH'\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", StringType(), False),\n",
    "    StructField(\"OBS_DATE\", DateType(), False),\n",
    "    StructField(\"ELEMENT\", StringType(), False),\n",
    "    StructField(\"DATA_VALUE\", IntegerType(), True),\n",
    "    StructField(\"M_FLAG\", StringType(), True),\n",
    "    StructField(\"Q_FLAG\", StringType(), True),\n",
    "    StructField(\"S_FLAG\", StringType(), True),\n",
    "    StructField(\"OBS_TIME\", StringType(), True)])\n",
    "df2 = spark.read.csv(\"s3://noaa-ghcn-pds/csv.gz/2018.csv.gz\",schema,dateFormat='yyyyMMdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_cmh_df = df2.filter((\"ID = 'USW00014821' and MONTH(OBS_DATE) = 7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_cmh_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_cmh_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "cmh_min_temps = (port_cmh_df.filter(\"ELEMENT = 'TMIN'\")\n",
    "   .withColumn('LOW_TEMP_C', port_cmh_df.DATA_VALUE / 10)\n",
    "   .withColumn('LOW_TEMP_F', format_number(port_cmh_df.DATA_VALUE * .18 + 32,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmh_min_temps.select('OBS_DATE', 'LOW_TEMP_C', 'LOW_TEMP_F').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmh_max_temps = (port_cmh_df.filter(\"ELEMENT = 'TMAX'\")\n",
    "   .withColumn('HIGH_TEMP_C', port_cmh_df.DATA_VALUE / 10)\n",
    "   .withColumn('HIGH_TEMP_F', format_number(port_cmh_df.DATA_VALUE * .18 + 32,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmh_max_temps.select('OBS_DATE', 'HIGH_TEMP_C', 'HIGH_TEMP_F').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmh_2018_07_temps = cmh_min_temps.select('OBS_DATE', 'LOW_TEMP_C', 'LOW_TEMP_F').join(cmh_max_temps.select('OBS_DATE', 'HIGH_TEMP_C', 'HIGH_TEMP_F'), 'OBS_DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmh_2018_07_temps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmh_2018_07_temps.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"s3://YOUR-BUCKET-NAME-HERE/cmh-temps-csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
