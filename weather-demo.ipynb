{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark with Jupyter\n",
    "\n",
    "## Prerequisites:\n",
    "\n",
    "* You completed [Launching an Elastic Map Reduce (EMR) Cluster](https://github.com/jeffrey-anderson/aws-intro-demos/blob/master/Demo-EMR-Launch.md)\n",
    "* Your custer is in a \"Waiting\" state\n",
    "\n",
    "    \n",
    "## References:\n",
    "\n",
    "*   [Apache Spark Documentation](http://spark.apache.org/)\n",
    "*   [Spark Python API Documentation](http://spark.apache.org/docs/latest/api/python/index.html)\n",
    "*   [AWS JupyterHub Documentation](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyterhub.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Spark application named \"weather\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('weather').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a data frame called \"df\" from the [ghcnd-stations.txt](https://docs.opendata.aws/noaa-ghcn-pds/readme.html) file in S3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"s3://noaa-ghcn-pds/ghcnd-stations.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dataframe called \"stations\" by splitting the fixed width data stored in the `df` dataframe into individual\n",
    "columns, trimming whitespace, specifying the proper type, and providing a column name for each:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "\n",
    "stations = df.select(\n",
    "    df.value.substr(1,11).alias('ID'),\n",
    "    df.value.substr(13,8).cast('double').alias('LATITUDE'),\n",
    "    df.value.substr(22,9).cast('double').alias('LONGITUDE'),\n",
    "    df.value.substr(32,6).cast('double').alias('ELEVATION'),\n",
    "    trim(df.value.substr(39,2)).alias('STATE'),\n",
    "    trim(df.value.substr(42,30)).alias('NAME'),\n",
    "    trim(df.value.substr(73,3)).alias('GSN_FLAG'),\n",
    "    trim(df.value.substr(77,3)).alias('NETWORK_FLAG'),\n",
    "    trim(df.value.substr(81,5)).alias('WMO_ID')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instruct Spark to cache the `stations` dataframe into memory for faster performance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the first 20 columns of the `stations` dataframe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create and show a transient dataframe filtered so the name contains \"COLUMBUS\" and the state is OH:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.filter((\"NAME like '%COLUMBUS%' AND STATE = 'OH'\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further refine the results so only the \"PORT COLUMBUS\" row is shown:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.filter((\"NAME like '%PORT COLUMBUS%' AND STATE = 'OH'\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a schema for the daily weather observation data and apply it while reading the 2018 file into a new dataframe called `df2`**\n",
    "\n",
    "*Note:* If you want to explore the full set of weather data, substitute `\"s3://noaa-ghcn-pds/csv.gz/\"` for the \n",
    "location but be aware the history is large so subsequent operations may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", StringType(), False),\n",
    "    StructField(\"OBS_DATE\", DateType(), False),\n",
    "    StructField(\"ELEMENT\", StringType(), False),\n",
    "    StructField(\"DATA_VALUE\", IntegerType(), True),\n",
    "    StructField(\"M_FLAG\", StringType(), True),\n",
    "    StructField(\"Q_FLAG\", StringType(), True),\n",
    "    StructField(\"S_FLAG\", StringType(), True),\n",
    "    StructField(\"OBS_TIME\", StringType(), True)])\n",
    "df2 = spark.read.csv(\"s3://noaa-ghcn-pds/csv.gz/2018.csv.gz\",schema,dateFormat='yyyyMMdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tell Spark to cache the results in memory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the results (*this may take a few minutes*):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a data frame called `port_cmh_df` which only has data for the month of July from the Port Columbus station`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_cmh_df = df2.filter((\"ID = 'USW00014821' and MONTH(OBS_DATE) = 7\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cache the results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_cmh_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_cmh_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new data frame called `cmh_min_temps` with two additional columns for celsius and fahrenheit low\n",
    "temperatures calculated from the DATA_VALUE from rows with an element of 'TMIN':**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "cmh_min_temps = (port_cmh_df.filter(\"ELEMENT = 'TMIN'\")\n",
    "   .withColumn('LOW_TEMP_C', port_cmh_df.DATA_VALUE / 10)\n",
    "   .withColumn('LOW_TEMP_F', format_number(port_cmh_df.DATA_VALUE * .18 + 32,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the new columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmh_min_temps.select('OBS_DATE', 'LOW_TEMP_C', 'LOW_TEMP_F').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a `cmh_max_temps` dataframe derived the same way for rows with an element of 'TMAX':**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmh_max_temps = (port_cmh_df.filter(\"ELEMENT = 'TMAX'\")\n",
    "   .withColumn('HIGH_TEMP_C', port_cmh_df.DATA_VALUE / 10)\n",
    "   .withColumn('HIGH_TEMP_F', format_number(port_cmh_df.DATA_VALUE * .18 + 32,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the new columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmh_max_temps.select('OBS_DATE', 'HIGH_TEMP_C', 'HIGH_TEMP_F').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new dataframe called `cmh_2018_07_temps` by joining the min and max temperature dataframes created above:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmh_2018_07_temps = cmh_min_temps.select('OBS_DATE', 'LOW_TEMP_C', 'LOW_TEMP_F').join(cmh_max_temps.select('OBS_DATE', 'HIGH_TEMP_C', 'HIGH_TEMP_F'), 'OBS_DATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the joined dataframe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmh_2018_07_temps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the dataframe to a CSV file in S3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmh_2018_07_temps.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"s3://YOUR-BUCKET-NAME-HERE/cmh-temps-csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
